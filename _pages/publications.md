---
title: "Publications"
layout: single
permalink: /publications/
classes:
  - wide
header:
  overlay_image: /assets/images/research-vision.jpg
  overlay_filter: 0.5
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---

<div class="publications-container">
  <div class="publications-intro">
    <p>Our lab members publish in top-tier conferences and journals in computer vision, machine learning, and multimedia analysis.</p>
    
    <div class="filter-years">
      <h3>Filter by Year:</h3>
      <div class="year-tabs">
        <a href="#2025" class="year-tab active">2025</a>
        <a href="#2024" class="year-tab">2024</a>
        <a href="#2023" class="year-tab">2023</a>
        <a href="#2022" class="year-tab">2022</a>
        <a href="#2021" class="year-tab">2021</a>
      </div>
    </div>

    <div class="publication-year" id="2025">
      <h2>2025</h2>
      
      <div class="publications-grid">
        <div class="publication-card" data-aos="fade-up">
          <h3>Transformer-based Neural Architecture for Multimodal Image Segmentation</h3>
          <p class="authors">Lee, J., Chen, T., Wang, H., & Zhang, Y.</p>
          <p class="venue">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025</p>
          <p class="abstract">This paper presents a novel transformer-based architecture for multimodal image segmentation that effectively fuses information from multiple sensors. Our approach achieves state-of-the-art performance on benchmark datasets while being computationally efficient.</p>
          <div class="publication-links">
            <a href="#" class="pub-link doi" target="_blank">
              <i class="fas fa-external-link-alt"></i>
              <span>DOI</span>
            </a>
            <a href="#" class="pub-link pdf" target="_blank">
              <i class="far fa-file-pdf"></i>
              <span>PDF</span>
            </a>
            <a href="#" class="pub-link code" target="_blank">
              <i class="fas fa-code"></i>
              <span>Code</span>
            </a>
          </div>
        </div>
        
        <div class="publication-card" data-aos="fade-up" data-aos-delay="100">
          <h3>Self-Supervised Learning for Medical Image Analysis with Limited Annotations</h3>
          <p class="authors">Wang, H., Zhang, Y., Lee, J., & Chen, T.</p>
          <p class="venue">Medical Image Analysis, 2025</p>
          <p class="abstract">We propose a self-supervised learning framework specifically designed for medical imaging applications where annotated data is scarce. Our method leverages anatomical priors to guide the learning process, resulting in more accurate segmentation and classification models.</p>
          <div class="publication-links">
            <a href="#" class="pub-link doi" target="_blank">
              <i class="fas fa-external-link-alt"></i>
              <span>DOI</span>
            </a>
            <a href="#" class="pub-link pdf" target="_blank">
              <i class="far fa-file-pdf"></i>
              <span>PDF</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    
    <div class="publication-year" id="2024">
      <h2>2024</h2>
      
      <div class="publications-grid">
        <div class="publication-card" data-aos="fade-up">
          <h3>Efficient Neural Networks for Real-time Video Understanding</h3>
          <p class="authors">Chen, T., Lee, J., Wang, H., & Zhang, Y.</p>
          <p class="venue">International Conference on Machine Learning (ICML), 2024</p>
          <p class="abstract">This work introduces a family of efficient neural networks designed for real-time video understanding tasks. Our architectures achieve competitive performance while requiring significantly less computational resources, making them suitable for deployment on edge devices.</p>
          <div class="publication-links">
            <a href="#" class="pub-link doi" target="_blank">
              <i class="fas fa-external-link-alt"></i>
              <span>DOI</span>
            </a>
            <a href="#" class="pub-link pdf" target="_blank">
              <i class="far fa-file-pdf"></i>
              <span>PDF</span>
            </a>
            <a href="#" class="pub-link code" target="_blank">
              <i class="fas fa-code"></i>
              <span>Code</span>
            </a>
          </div>
        </div>
        
        <div class="publication-card" data-aos="fade-up" data-aos-delay="100">
          <h3>Vision-Language Models for Automated Medical Report Generation</h3>
          <p class="authors">Zhang, Y., Wang, H., Chen, T., & Lee, J.</p>
          <p class="venue">IEEE Transactions on Medical Imaging, 2024</p>
          <p class="abstract">We present an end-to-end vision-language model that can automatically generate accurate and detailed medical reports from radiological images. Our model incorporates domain knowledge and can explain its reasoning, improving trust and adoption in clinical settings.</p>
          <div class="publication-links">
            <a href="#" class="pub-link doi" target="_blank">
              <i class="fas fa-external-link-alt"></i>
              <span>DOI</span>
            </a>
            <a href="#" class="pub-link pdf" target="_blank">
              <i class="far fa-file-pdf"></i>
              <span>PDF</span>
            </a>
          </div>
        </div>
      </div>
    </div>
    
    <div class="publication-year" id="2023">
      <h2>2023</h2>
      
      <div class="publications-grid">
        <div class="publication-card" data-aos="fade-up">
          <h3>Contrastive Learning for Unsupervised Video Representation</h3>
          <p class="authors">Chen, T., Zhang, Y., Wang, H., & Lee, J.</p>
          <p class="venue">International Conference on Computer Vision (ICCV), 2023</p>
          <p class="abstract">This work explores contrastive learning approaches for learning robust video representations without human supervision. By leveraging temporal consistency and multi-view information, our method achieves competitive performance on downstream tasks with minimal labeled data.</p>
          <div class="publication-links">
            <a href="#" class="pub-link doi" target="_blank">
              <i class="fas fa-external-link-alt"></i>
              <span>DOI</span>
            </a>
            <a href="#" class="pub-link pdf" target="_blank">
              <i class="far fa-file-pdf"></i>
              <span>PDF</span>
            </a>
            <a href="#" class="pub-link code" target="_blank">
              <i class="fas fa-code"></i>
              <span>Code</span>
            </a>
          </div>
        </div>
        
        <div class="publication-card" data-aos="fade-up" data-aos-delay="100">
          <h3>Generative Adversarial Networks for Data Augmentation in Medical Imaging</h3>
          <p class="authors">Wang, H., Chen, T., Lee, J., & Zhang, Y.</p>
          <p class="venue">Neural Information Processing Systems (NeurIPS), 2023</p>
          <p class="abstract">We develop a novel GAN architecture specifically designed for generating realistic medical images that can be used for data augmentation. Our approach addresses the challenge of limited training data in medical image analysis by generating diverse, anatomically correct samples.</p>
          <div class="publication-links">
            <a href="#" class="pub-link doi" target="_blank">
              <i class="fas fa-external-link-alt"></i>
              <span>DOI</span>
            </a>
            <a href="#" class="pub-link pdf" target="_blank">
              <i class="far fa-file-pdf"></i>
              <span>PDF</span>
            </a>
            <a href="#" class="pub-link code" target="_blank">
              <i class="fas fa-code"></i>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="publications-stats">
  <div class="stats-container">
    <h2>Publication Statistics</h2>
    
    <div class="stats-grid">
      <div class="stat-card" data-aos="fade-up">
        <div class="stat-number">24+</div>
        <div class="stat-label">Journal Articles</div>
      </div>
      
      <div class="stat-card" data-aos="fade-up" data-aos-delay="100">
        <div class="stat-number">45+</div>
        <div class="stat-label">Conference Papers</div>
      </div>
      
      <div class="stat-card" data-aos="fade-up" data-aos-delay="200">
        <div class="stat-number">12+</div>
        <div class="stat-label">Patents</div>
      </div>
      
      <div class="stat-card" data-aos="fade-up" data-aos-delay="300">
        <div class="stat-number">1500+</div>
        <div class="stat-label">Citations</div>
      </div>
    </div>
  </div>
</div>