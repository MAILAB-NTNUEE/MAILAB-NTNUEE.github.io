var store = [{
        "title": "CVPR 2025 Best Paper Award",
        "excerpt":"We are thrilled to announce that our paper “Advanced Vision Transformers for Multimodal Understanding” has been awarded the prestigious Best Paper Award at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025.   About the Research   Our research introduces a novel transformer-based architecture that significantly improves the performance of computer vision systems in understanding complex scenes with multiple objects and interactions. The key innovations include:      A new self-attention mechanism that efficiently processes high-resolution images   Multi-scale feature integration that captures both fine details and global context   A cross-modal alignment approach that bridges visual and linguistic representations   Impact and Applications   This work represents a major advancement in computer vision and has immediate applications in:      Autonomous driving systems with improved scene understanding   Medical imaging analysis with higher accuracy and interpretability   Assistive technologies for visually impaired individuals   Advanced robotics with improved environmental perception   Team Recognition   This achievement is the result of collaborative efforts from our talented team members:      Dr. [Lead Researcher]   [PhD Student 1]   [PhD Student 2]   [Collaborator] from [Partner University]   Media Coverage   Our work has received significant media attention, with features in:      [Tech Publication]   [Science Magazine]   [University Press Release]   Next Steps   Building on this research, we are now exploring applications in real-time video understanding and expanding our approach to handle more complex multi-modal interactions. Stay tuned for more updates on this exciting research direction.   The full paper is available on arXiv and will be presented at CVPR 2025 in Los Angeles.  ","categories": ["Awards"],
        "tags": ["computer-vision","deep-learning","publications","awards","conferences"],
        "url": "/news/2025-04-01-cvpr-best-paper/",
        "teaser": "/assets/images/news/cvpr-award-teaser.jpg"
      },{
        "title": "MAILAB Researchers Present at IEEE International Conference on Robotics and Automation",
        "excerpt":"This week, three members of our lab presented their cutting-edge research at the IEEE International Conference on Robotics and Automation (ICRA) 2025 in Stockholm, Sweden.   Research Presentations   Our team showcased two papers that highlight our recent advances in robotic perception:   1. “Real-time 6D Object Pose Estimation for Robotic Manipulation”   This paper introduces a novel deep learning architecture that can accurately estimate the 6D pose of objects in cluttered environments with previously unseen objects. The system achieves state-of-the-art performance while maintaining real-time processing speeds suitable for dynamic robotic manipulation tasks.   Presenters: [PhD Student 1] and [PhD Student 2]   2. “Self-Supervised Visual-Tactile Learning for Robotic Grasping”   This work demonstrates a multimodal approach that combines visual and tactile information to improve grasping success rates for objects with complex geometries or deformable properties. The self-supervised learning framework allows robots to improve their grasping capabilities through interaction, without requiring extensive manual labeling.   Presenter: [PhD Student 3]   Collaboration Opportunities   The conference provided valuable opportunities to connect with researchers from leading robotics labs and companies worldwide. Several potential collaborations were discussed with teams from:      [University 1] on combining our visual perception models with their motion planning algorithms   [Company 1] on potential industrial applications of our grasping technology   [Research Institute] on extending our work to mobile manipulation scenarios   Demo Highlights   Our researchers also participated in the demonstration session, where they showcased a live demo of our robotic system performing grasp detection and execution on challenging everyday objects with varying shapes, sizes, and material properties.   The demo received significant attention from both academic researchers and industry representatives, highlighting the practical applicability of our research.   Looking Forward   The feedback received during the conference has provided valuable insights for refining our approaches and identifying new research directions. Our team is now working on incorporating these insights into our ongoing projects.   We look forward to expanding on this research and presenting new advances at upcoming robotics conferences.   For more details about this research, please contact the presenting authors or visit our research page.  ","categories": ["Events"],
        "tags": ["robotics","computer-vision","presentations","conferences","research"],
        "url": "/news/2025-04-10-icra-conference-presentations/",
        "teaser": "/assets/images/news/icra-2025-teaser.jpg"
      },{
        "title": "Template: New Research Grant Awarded",
        "excerpt":"We’re excited to announce that our lab has been awarded a major research grant from the National Science Foundation to support our work on computer vision algorithms for autonomous systems.   Project Overview   The $500,000 grant will fund our research on developing more efficient and accurate computer vision algorithms for real-time object detection and tracking in autonomous vehicles.   Research Impact   This funding will support:      Two PhD students for the next three years   New GPU computing infrastructure   Collaboration with industry partners   Next Steps   We’re currently recruiting talented graduate students interested in computer vision and deep learning to join this project. Learn more about opportunities here.   About the Funding   This research is supported by NSF Grant No. 2025-12345. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.   ","categories": ["Funding"],
        "tags": ["grants","computer-vision","research"],
        "url": "/news/template-news-item/",
        "teaser": "/assets/images/news-default.jpg"
      },{
        "title": "news update_describtion",
        "excerpt":"Let me summarize what’s now in place in your website:      Complete News Collection Framework: You now have a fully functioning news system based on Jekyll collections with:            A dedicated _news directory with sample content       A template file that team members can copy to create new entries       Custom layouts and styling for beautiful presentation       Filtering capabilities by tags and categories                Recent Sample News Items: I’ve created several example news items based on your existing posts and added a new one (the ICRA conference presentation) to show how recent updates would be added.       Responsive Design: The news layout works well on all devices and includes visual elements like card layouts, date badges, category tags, and featured images.   How to Use the System Going Forward   For your team to add news, they should:      Copy the Template: Duplicate template-news-item.md or any existing news item   Rename the File: Follow the YYYY-MM-DD-title-with-hyphens.md format   Edit the Front Matter: Change title, date, categories, tags, and excerpt   Write Content: Add markdown content for the news item   Add Images: Place images in news and reference them   The beauty of this system is that anyone on your team, regardless of web design experience, can add news items by simply creating markdown files. Jekyll automatically handles all the display formatting based on the templates and styles we’ve set up.   Images for News Items   For a fully polished look, you’ll want to add images for your news items. I’ve set up the directory structure for these with news. For each news item, consider adding:      A featured image (larger, used at the top of the article page)   A teaser image (smaller, used for card displays on the main news page)   Next Steps for You      Share the Guide: Make sure your team knows about the ADDING_NEWS_GUIDE.md file   Add Real Images: Replace the placeholder image references with actual images for your news items   Customize Tags &amp; Categories: Adjust the suggested categories and tags in the guide to match your lab’s focus areas   Would you like me to explain any other aspect of the news system? Or would you like me to make any adjustments to the implementation?  ","categories": ["Description"],
        "tags": ["guided content"],
        "url": "/news/2025-04-11-update_describtion/",
        "teaser": null
      },{
        "title": "website launch",
        "excerpt":"test  website is updating and testing. Soon to Coming.  ","categories": [],
        "tags": [],
        "url": "/news/2025-04-11-website-launch/",
        "teaser": null
      },{
        "title": "New Research Project: Advanced Visual Recognition Models for Medical Imaging",
        "excerpt":"Our lab has launched a new research project exploring advanced visual recognition models for medical imaging. This project aims to develop deep learning algorithms that can assist radiologists in diagnosing various conditions from X-rays, CT scans, and MRIs.   The team will be focusing on creating models that can not only detect abnormalities but also provide explanations for their predictions, making them more trustworthy for clinical use. This research is funded by a new grant from the National Science Foundation.   The project will combine computer vision techniques with natural language processing to generate comprehensive reports from medical images, similar to those written by radiologists. We believe this approach will significantly reduce the time needed for diagnosis while maintaining high accuracy.   Graduate students interested in working on this project should contact Professor Chen for potential opportunities.  ","categories": ["Research"],
        "tags": ["computer-vision","medical-imaging","deep-learning"],
        "url": "/research/2025/03/15/medical-imaging-project.html",
        "teaser": "/assets/images/news/medical-vision.jpg"
      },{
        "title": "MAILAB Team Wins Best Paper Award at CVPR 2025",
        "excerpt":"We’re proud to announce that our lab’s paper “Self-supervised Learning for Resource-Constrained Visual Recognition” has received the Best Paper Award at the 2025 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).   The paper, authored by Dr. Sarah Kim, Robert Zhang, and Professor Jane Chen, presents a novel approach to training deep learning models with limited labeled data, achieving state-of-the-art performance while requiring significantly fewer computational resources.   CVPR is one of the most prestigious conferences in the field of computer vision, and this recognition highlights our lab’s commitment to pushing the boundaries of what’s possible in visual recognition systems.   The award committee praised the work for its innovative methodology and potential impact on making advanced computer vision accessible in resource-constrained environments like mobile devices and IoT applications.  ","categories": ["Awards"],
        "tags": ["awards","computer-vision","conferences"],
        "url": "/awards/2025/04/01/cvpr-best-paper.html",
        "teaser": "/assets/images/news/award.jpg"
      },{
        "title": "Workshop on Multimodal Learning: Registration Open",
        "excerpt":"MAILAB is hosting a two-day workshop on “Multimodal Learning for Computer Vision Applications” on May 15-16, 2025. This workshop will bring together researchers and practitioners to discuss the latest advances in combining visual, textual, and audio data for comprehensive AI systems.   Workshop Details      Dates: May 15-16, 2025   Location: Technology Building, NTNU Campus   Registration: Open until May 1st or until capacity is reached   Featured Speakers      Professor Alex Johnson (National Taiwan University)   Dr. Michelle Chen (Google Research)   Dr. Kevin Liu (NVIDIA)   Professor Jane Chen (NTNU)   Topics   The workshop will cover multimodal fusion techniques, transformer architectures for cross-modal understanding, self-supervised learning with multimodal data, and applications in healthcare, autonomous systems, and content creation.   Registration is now open on our website. Early registration is encouraged as space is limited to 50 participants.  ","categories": ["Events"],
        "tags": ["events","workshop","multimodal-learning"],
        "url": "/events/2025/04/05/multimodal-workshop.html",
        "teaser": "/assets/images/news/workshop.jpg"
      },]
